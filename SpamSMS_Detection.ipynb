{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a seq to one problem<br>\n",
    "Process:<br>\n",
    "1:Load, clean the data and tokenize<br>\n",
    "2:Encode the sentences (Create dictionary of words, map words to integers using Keras). Keras is only used for pre-processing, to build our model we used Tensorflow <br>\n",
    "3:Word embedding<br>\n",
    "4:Build RNN model (Create embddings and LSTM layers)<br>\n",
    "5:Run and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os.path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import string\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and clean the messages as well as encoding the lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_clean(filepath):\n",
    "    '''Load & clean the data'''\n",
    "\n",
    "    #Loading data\n",
    "    data = pd.read_csv(filepath)\n",
    "    #rows_number=data.shape[0]\n",
    "    messages=[]\n",
    "    for message in data['v2']:\n",
    "        #Extra cleaning of text before Keras tokenization \n",
    "        #Removing stopwords\n",
    "        nltk.download(\"stopwords\")\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        message=' '.join(i for i in message.split() if i not in stop_words)\n",
    "        #Here, BeauifulSoup is used to encode the not completely deccoded text(decoded from html code) to html code again\n",
    "        message = BeautifulSoup(message, 'lxml')\n",
    "        #Later we strip away tags in the html encodings and decode them to text\n",
    "        message=message.get_text()\n",
    "        messages.append(message)\n",
    "    \n",
    "    #Encoding labels\n",
    "    labels=[]\n",
    "    [labels.append(0) if label==\"spam\" else labels.append(1) for label in data['v1']]\n",
    "    labels = np.asarray(labels)\n",
    "    return messages,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize sentences and encode their words to integers, Keras is helpful here for normalization, tokenization as well as generating word indexes and padding the sentences (make sentences to have the same lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_words(sentences):\n",
    "    '''Convert words to numbers (Create the dictionary of words)'''\n",
    "    \n",
    "    #Since we read from csv, we need to do some encoding\n",
    "    #Remove u'\n",
    "    sentences=[x.encode('utf-8') for x in sentences]\n",
    "    #Remove \\xHH characters\n",
    "    sentences=[re.sub(r'[^\\x00-\\x7f]',r'', x) for x in sentences]\n",
    "    \n",
    "    #Keras tokenization (punctuation removal, normalization and split by white space)\n",
    "    tokenize = Tokenizer()\n",
    "    #Fit tokenizer to the whole data\n",
    "    tokenize.fit_on_texts(sentences)\n",
    "    data_seq=tokenize.texts_to_sequences(sentences)\n",
    "    word_index = tokenize.word_index\n",
    "    #Choose the maximum number of tokens in all sequences \n",
    "    num_tokens = [len(tokens) for tokens in data_seq]\n",
    "    max_seq_length=np.max(num_tokens)\n",
    "    #Make sequences to have the same lengths (add extra zeros to the beginnings of the sentences)\n",
    "    data_seq = pad_sequences(data_seq, maxlen = max_seq_length,\n",
    "                                padding='pre', truncating='pre')\n",
    "    #print(data_seq)\n",
    "    return data_seq,word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    '''Define model inputs'''\n",
    "    \n",
    "    #Resert the default graph \n",
    "    tf.reset_default_graph()\n",
    "    #Model's placeholders for inputs\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    return inputs,targets,keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(inputs,vocabulary_size,embedding_size):\n",
    "    '''Intialize embeddings for the words. Embedding layer connects the words to the LSTM layers (words are embedded to the embedding_size vectors instead of vocabulary size vectors or one hot vectors which aren't efficient). Here, we used random_uniform distribution to initialize the words' embeddings and then they are trained by the model to have more meaningful embeddings'''\n",
    "    \n",
    "    #Embedding Layer\n",
    "    embedding = tf.Variable(tf.random_uniform((vocabulary_size, embedding_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "    \n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the RNN model with 2 LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_RNN(inputs,num_hidden,lstm_layer_numbers,keep_prob,batch_size):\n",
    "    '''Build RNN cells'''\n",
    "\n",
    "    #Define LSTM layers\n",
    "    lstms=[]\n",
    "    for i in range(lstm_layer_numbers):\n",
    "        lstms.append(tf.contrib.rnn.BasicLSTMCell(num_hidden))\n",
    "    #Add regularization dropout to the LSTM cells\n",
    "    drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob) for lstm in lstms]\n",
    "    #Stack up multiple LSTM layers\n",
    "    stacked_lstm = tf.contrib.rnn.MultiRNNCell(drops)\n",
    "    #Getting the initial state\n",
    "    initial_state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "    return initial_state, stacked_lstm\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size):\n",
    "    '''Using generator to return batches for train, validation and test data'''\n",
    "\n",
    "    n_batches = len(x)//batch_size\n",
    "    '''In case that the batch_size is not a multiple of data size in order to create batches with the same sizes, this line will ignore the last words in text that cannot create a full batch (Although one can consider those last words and add enough words from the beginning of the text to create a full size batch)'''\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input data\n",
    "emaildata_file=\"./spam.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading and cleaning the data; return clean messages and labels\n",
    "text_messages,labels=load_clean(emaildata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ..., 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words to numbers\n",
    "data_sequences,word_index=encode_words(text_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raining': 1592,\n",
       " 'yellow': 4011,\n",
       " 'four': 2311,\n",
       " 'prices': 6549,\n",
       " 'woods': 6739,\n",
       " \"friend's\": 2388,\n",
       " 'hanging': 1973,\n",
       " 'looking': 396,\n",
       " 'electricity': 3752,\n",
       " 'scold': 3754,\n",
       " 'lord': 6828,\n",
       " 'rp176781': 4605,\n",
       " 'callin': 2480,\n",
       " 'ew': 6749,\n",
       " 'hearin': 8343,\n",
       " 'screaming': 1608,\n",
       " 'disturb': 1093,\n",
       " 'prize': 107,\n",
       " 'andre': 8476,\n",
       " 'smsing': 7980,\n",
       " 'wednesday': 1274,\n",
       " 'oooh': 3205,\n",
       " 'specially': 1072,\n",
       " 'nigh': 7532,\n",
       " 'tired': 809,\n",
       " 'snuggles': 8160,\n",
       " \"'wnevr\": 6818,\n",
       " 'second': 621,\n",
       " 'attended': 7746,\n",
       " 'txtno': 3131,\n",
       " 'available': 616,\n",
       " 'scraped': 8165,\n",
       " '2kbsubject': 4899,\n",
       " 'scallies': 7419,\n",
       " 'errors': 5266,\n",
       " 'cooking': 2231,\n",
       " 'fingers': 1223,\n",
       " 'maraikara': 6845,\n",
       " 'hero': 5162,\n",
       " \"how've\": 6751,\n",
       " 'y87': 6931,\n",
       " 'here': 233,\n",
       " 'specialise': 5727,\n",
       " '47': 7730,\n",
       " 'china': 2793,\n",
       " 'dogwood': 7964,\n",
       " 'dorm': 3261,\n",
       " '08718711108': 4829,\n",
       " 'previews': 5968,\n",
       " '84122': 5275,\n",
       " 'w111wx': 2211,\n",
       " 'kids': 1035,\n",
       " '84128': 2631,\n",
       " 'eastenders': 3427,\n",
       " '09058091870': 7880,\n",
       " \"i'd\": 854,\n",
       " \"i'm\": 6,\n",
       " 'spotty': 6044,\n",
       " 'golden': 6590,\n",
       " \"ta's\": 4419,\n",
       " \"dat's\": 3171,\n",
       " 'replace': 4343,\n",
       " 'brought': 2172,\n",
       " 'sterm': 8621,\n",
       " '000pes': 5744,\n",
       " 'txt': 28,\n",
       " 'univ': 7383,\n",
       " '9t': 3161,\n",
       " 'cheating': 4109,\n",
       " 'spoke': 1477,\n",
       " 'ec2a': 1668,\n",
       " 'browse': 5573,\n",
       " 'dnt': 818,\n",
       " 'music': 485,\n",
       " 'passport': 7259,\n",
       " 'strike': 2873,\n",
       " 'until': 7549,\n",
       " 'paperwork': 3094,\n",
       " 'holy': 3815,\n",
       " 'relax': 1506,\n",
       " 'successful': 2544,\n",
       " 'brings': 1106,\n",
       " 'premarica': 6284,\n",
       " 'hols': 1913,\n",
       " 'yahoo': 1209,\n",
       " 'hurt': 528,\n",
       " '99': 3455,\n",
       " 'glass': 5361,\n",
       " '47per': 5980,\n",
       " 'hole': 5020,\n",
       " 'hold': 776,\n",
       " '95': 5385,\n",
       " 'up4': 3704,\n",
       " 'tirupur': 2475,\n",
       " 'itself': 1480,\n",
       " 'wana': 943,\n",
       " 'drvgsto': 4901,\n",
       " 'pints': 7613,\n",
       " 'smth': 681,\n",
       " 'want': 26,\n",
       " 'organizer': 5614,\n",
       " 'preferably': 2167,\n",
       " 'hon': 3957,\n",
       " 'hoo': 7609,\n",
       " 'travel': 1772,\n",
       " 'how': 31,\n",
       " 'hot': 526,\n",
       " 'hor': 2937,\n",
       " 'hos': 8379,\n",
       " 'hop': 1311,\n",
       " 'significance': 4694,\n",
       " '1172': 8208,\n",
       " 'beauty': 4255,\n",
       " 'yun': 1794,\n",
       " 'wan2': 4127,\n",
       " 'plyr': 4966,\n",
       " 'wrong': 748,\n",
       " 'lololo': 6073,\n",
       " 'bsnl': 6320,\n",
       " 'types': 5465,\n",
       " 'ibored': 8519,\n",
       " 'aroundn': 6954,\n",
       " 'wins': 1685,\n",
       " 'yunny': 3934,\n",
       " 'alian': 6227,\n",
       " 'age16': 1119,\n",
       " 'tulip': 3430,\n",
       " 'areyouunique': 4496,\n",
       " 'keeps': 3683,\n",
       " 'lambda': 5889,\n",
       " 'wind': 3459,\n",
       " 'wine': 932,\n",
       " 'wc1n': 8783,\n",
       " 'afterwards': 8455,\n",
       " \"dramastorm's\": 6465,\n",
       " 'vary': 1513,\n",
       " 'kickoff': 3422,\n",
       " '82050': 3720,\n",
       " '87575': 1894,\n",
       " 'welcomes': 4028,\n",
       " 'lovingly': 3637,\n",
       " 'fit': 2374,\n",
       " 'bringing': 2011,\n",
       " 'fix': 1686,\n",
       " 'max10mins': 1621,\n",
       " '4eva': 4334,\n",
       " 'matured': 2014,\n",
       " '09095350301': 8642,\n",
       " 'wales': 2276,\n",
       " 'hidden': 7726,\n",
       " 'nokia6600': 3712,\n",
       " 'easier': 2126,\n",
       " 'duvet': 8052,\n",
       " 'vouchers': 550,\n",
       " 'effects': 3234,\n",
       " 'schools': 3287,\n",
       " 'go2sri': 7321,\n",
       " 'silver': 3567,\n",
       " 'rumour': 3624,\n",
       " 'fetching': 4812,\n",
       " 'dload': 1790,\n",
       " 'nattil': 7273,\n",
       " 'arrow': 6192,\n",
       " 'addicted': 2090,\n",
       " 'burial': 4981,\n",
       " 'financial': 6226,\n",
       " 'fgkslpopw': 8247,\n",
       " 'series': 1905,\n",
       " 'allah': 1700,\n",
       " 'spider': 4359,\n",
       " 'bowls': 6694,\n",
       " 'strips': 5240,\n",
       " \"we'd\": 1938,\n",
       " '2day': 1099,\n",
       " 'ring': 576,\n",
       " 'rt': 5660,\n",
       " 'ru': 1868,\n",
       " 'rv': 4982,\n",
       " 'forwarding': 5961,\n",
       " 'rr': 6330,\n",
       " 'rs': 726,\n",
       " 'ha': 680,\n",
       " 'help08714742804': 5520,\n",
       " 'sms': 193,\n",
       " 'rd': 1838,\n",
       " 're': 1404,\n",
       " 'noice': 7627,\n",
       " '09061701851': 7438,\n",
       " 'ofcourse': 8750,\n",
       " 'dracula': 3654,\n",
       " 'toking': 7994,\n",
       " 'sheet': 6317,\n",
       " 'ate': 1573,\n",
       " 'shelves': 6135,\n",
       " 'atm': 1679,\n",
       " 'ups': 4525,\n",
       " 'shipped': 3115,\n",
       " \"today's\": 1354,\n",
       " 'clothes': 4247,\n",
       " 'veggie': 5286,\n",
       " 'kfc': 7742,\n",
       " 'hear': 349,\n",
       " 'ente': 5168,\n",
       " \"b'tooth\": 5844,\n",
       " 'basketball': 7288,\n",
       " 'service': 177,\n",
       " '09061743386': 4219,\n",
       " 'engagement': 8080,\n",
       " 'xin': 7300,\n",
       " 'needed': 1921,\n",
       " 'listed': 7291,\n",
       " 'loosu': 7777,\n",
       " 'hiya': 1135,\n",
       " 'listen': 787,\n",
       " 'clubmoby': 8125,\n",
       " 'wisdom': 2906,\n",
       " 'termsapply': 8175,\n",
       " 'trek': 4870,\n",
       " 'peril': 7090,\n",
       " 'showed': 7262,\n",
       " 'saeed': 5998,\n",
       " 'tree': 2421,\n",
       " 'likely': 2478,\n",
       " 'project': 779,\n",
       " 'percentages': 7083,\n",
       " 'bridgwater': 4796,\n",
       " 'feeling': 531,\n",
       " 'boston': 1658,\n",
       " '09061749602': 5093,\n",
       " 'selflessness': 6780,\n",
       " '9755': 4709,\n",
       " '9758': 8158,\n",
       " 'affairs': 2922,\n",
       " 'escalator': 5149,\n",
       " 'flippin': 5294,\n",
       " 'responsible': 5679,\n",
       " 'witot': 5451,\n",
       " 'andros': 4056,\n",
       " 'okie': 605,\n",
       " 'causing': 4158,\n",
       " 'doors': 2697,\n",
       " 'hum': 6209,\n",
       " 'shall': 357,\n",
       " 'doin': 1057,\n",
       " 'victoria': 8684,\n",
       " 'doit': 5023,\n",
       " 'swiss': 3914,\n",
       " 'laxinorficated': 7144,\n",
       " 'mouth': 3019,\n",
       " 'daywith': 6776,\n",
       " 'letter': 1900,\n",
       " 'thriller': 6909,\n",
       " 'cops': 8710,\n",
       " 'marsms': 6060,\n",
       " 'camp': 7690,\n",
       " 'passes': 7392,\n",
       " 'everythin': 5356,\n",
       " '41685': 3342,\n",
       " 'tech': 3392,\n",
       " '84484': 7572,\n",
       " 'scream': 1531,\n",
       " 'came': 427,\n",
       " 'marvel': 8753,\n",
       " 'saying': 584,\n",
       " 'bomb': 8270,\n",
       " 'prin': 7671,\n",
       " 'insects': 5372,\n",
       " 'advisors': 8363,\n",
       " 'teresa': 5814,\n",
       " 'prix': 4782,\n",
       " 'gauge': 5430,\n",
       " 'buzzzz': 6696,\n",
       " 'participate': 6803,\n",
       " 'lessons': 1087,\n",
       " 'busy': 544,\n",
       " \"u'll\": 1399,\n",
       " 'menu': 1309,\n",
       " 'appreciated': 3485,\n",
       " 'cougar': 5177,\n",
       " 'touched': 2348,\n",
       " 'rich': 2784,\n",
       " 'rice': 3385,\n",
       " 'pocked': 7810,\n",
       " 'plate': 6968,\n",
       " '0871277810810': 3020,\n",
       " 'platt': 6042,\n",
       " 'uworld': 8579,\n",
       " 'tips': 7657,\n",
       " 'lmao': 1422,\n",
       " 'bus8': 5136,\n",
       " 'kittum': 7274,\n",
       " 'asssssholeeee': 7566,\n",
       " 'piggy': 4843,\n",
       " 'respond': 1719,\n",
       " 'disaster': 8376,\n",
       " 'fair': 2286,\n",
       " 'rupaul': 5949,\n",
       " 'goodnight': 1089,\n",
       " 'result': 2004,\n",
       " 'bleh': 3374,\n",
       " 'best': 241,\n",
       " 'lotz': 8212,\n",
       " 'ctargg': 5041,\n",
       " 'lots': 653,\n",
       " 'lotr': 2686,\n",
       " 'wikipedia': 4826,\n",
       " '80122300p': 6401,\n",
       " 'stamps': 2592,\n",
       " 'score': 2624,\n",
       " 'glasgow': 5570,\n",
       " 'men': 1070,\n",
       " 'nationwide': 7573,\n",
       " 'nature': 1605,\n",
       " 'rolled': 6416,\n",
       " 'rajini': 5517,\n",
       " 'icicibank': 3559,\n",
       " 'wtc': 6599,\n",
       " 'wtf': 1987,\n",
       " 'wth': 7375,\n",
       " 'roller': 8098,\n",
       " 'pity': 8816,\n",
       " 'accident': 2905,\n",
       " 'brown': 6002,\n",
       " 'country': 1302,\n",
       " 'macedonia': 4371,\n",
       " 'planned': 1449,\n",
       " 'lookin': 1611,\n",
       " 'tomarrow': 2346,\n",
       " 'machan': 1908,\n",
       " 'login': 1372,\n",
       " 'argue': 1957,\n",
       " 'asked': 415,\n",
       " '30th': 4495,\n",
       " 'itried2tell': 5791,\n",
       " '2nd': 382,\n",
       " 'happenin': 5396,\n",
       " 'darlin': 725,\n",
       " 'sk38xh': 1396,\n",
       " '250': 634,\n",
       " '255': 8153,\n",
       " '9ja': 2453,\n",
       " 'billing': 7901,\n",
       " 'shouting': 5583,\n",
       " 'fri': 742,\n",
       " 'fro': 5085,\n",
       " 'frm': 784,\n",
       " 'much': 74,\n",
       " 'wuld': 4031,\n",
       " 'stadium': 6568,\n",
       " 'parents': 720,\n",
       " 'obese': 5207,\n",
       " 'life': 119,\n",
       " 'dave': 2450,\n",
       " 'lift': 1345,\n",
       " 'chile': 7463,\n",
       " 'child': 2280,\n",
       " '25p': 1235,\n",
       " 'spin': 7802,\n",
       " 'bridal': 7134,\n",
       " 'chill': 2647,\n",
       " 'unsold': 1774,\n",
       " '3680': 3072,\n",
       " '09058091854': 3435,\n",
       " '2wks': 3058,\n",
       " 'selfindependence': 7544,\n",
       " 'meetin': 2059,\n",
       " 'k': 52,\n",
       " '42810': 8263,\n",
       " 'a21': 5702,\n",
       " 'congratulation': 8350,\n",
       " 'played': 2086,\n",
       " '078': 8513,\n",
       " 'player': 719,\n",
       " 'feed': 8682,\n",
       " 'things': 204,\n",
       " 'honi': 7599,\n",
       " 'tgxxrz': 4704,\n",
       " 'dha': 5971,\n",
       " 'hont': 7711,\n",
       " 'split': 8091,\n",
       " 'babies': 3564,\n",
       " '4fil': 3471,\n",
       " '08718727870150ppm': 8089,\n",
       " 'tops': 8008,\n",
       " 'ppm150': 5090,\n",
       " 'tune': 4942,\n",
       " 'academic': 4278,\n",
       " 'nachos': 7576,\n",
       " 'xxxxxxxxxxxxxx': 7177,\n",
       " 'opinions': 3869,\n",
       " 'gigolo': 7674,\n",
       " '08701752560': 7225,\n",
       " 'dosomething': 6448,\n",
       " 'sleepy': 3991,\n",
       " 'nydc': 3511,\n",
       " '87121': 1888,\n",
       " 'credited': 2121,\n",
       " 'waht': 3601,\n",
       " 'rushing': 8387,\n",
       " 'previous': 2929,\n",
       " 'hai': 1391,\n",
       " 'enters': 4675,\n",
       " 'ham': 2312,\n",
       " 'duffer': 5893,\n",
       " '1lemon': 7852,\n",
       " 'had': 405,\n",
       " 'haf': 497,\n",
       " 'obedient': 6497,\n",
       " 'innocent': 3952,\n",
       " 'east': 4094,\n",
       " 'hat': 8795,\n",
       " 'hav': 447,\n",
       " \"t's\": 1229,\n",
       " 'fromm': 2252,\n",
       " 'possible': 1127,\n",
       " 'twinks': 7418,\n",
       " 'possibly': 6734,\n",
       " 'birth': 2770,\n",
       " 'vday': 3744,\n",
       " 'shadow': 5996,\n",
       " 'unique': 1945,\n",
       " 'stylist': 5625,\n",
       " 'remind': 1863,\n",
       " 'steps': 7512,\n",
       " '9280114': 8148,\n",
       " 'ola': 3512,\n",
       " 'right': 110,\n",
       " 'old': 568,\n",
       " 'crowd': 7909,\n",
       " 'people': 225,\n",
       " 'weds': 6114,\n",
       " 'oli': 5353,\n",
       " 'easy': 373,\n",
       " 'feel': 162,\n",
       " 'fuuuuck': 8557,\n",
       " 'creep': 4002,\n",
       " 'enemies': 7488,\n",
       " '08718725756': 6901,\n",
       " 'for': 172,\n",
       " 'bottom': 2883,\n",
       " 'fox': 6788,\n",
       " 'creative': 7433,\n",
       " 'treadmill': 8590,\n",
       " 'muhommad': 7660,\n",
       " 'wocay': 8686,\n",
       " 'suitemates': 7578,\n",
       " 'dental': 8808,\n",
       " \"hubby's\": 8032,\n",
       " 'colleg': 7373,\n",
       " 'starring': 6452,\n",
       " 'losing': 2021,\n",
       " 'memorable': 4431,\n",
       " 'quiteamuzing': 8073,\n",
       " 'dollars': 1720,\n",
       " 'careabout': 5793,\n",
       " 'o': 838,\n",
       " 'suggestions': 8817,\n",
       " 'slightly': 3118,\n",
       " 'raised': 6170,\n",
       " 'statements': 6370,\n",
       " 'honeymoon': 5645,\n",
       " 'sol': 1434,\n",
       " 'soo': 3325,\n",
       " 'sos': 7536,\n",
       " '69696': 3971,\n",
       " '69698': 3065,\n",
       " 'soz': 8143,\n",
       " 'janx': 5401,\n",
       " '4742': 1942,\n",
       " 'support': 845,\n",
       " 'constantly': 2755,\n",
       " 'halla': 5528,\n",
       " 'greatness': 5346,\n",
       " 'jane': 2640,\n",
       " 'happy': 84,\n",
       " 'b4280703': 3169,\n",
       " 'offer': 330,\n",
       " '6wu': 3438,\n",
       " 'paypal': 5099,\n",
       " 'notifications': 4968,\n",
       " 'talents': 5671,\n",
       " 'fiting': 7662,\n",
       " 'congratulations': 731,\n",
       " 'inside': 1426,\n",
       " 'pest': 5621,\n",
       " 'lays': 3247,\n",
       " 'smashed': 3805,\n",
       " '151': 8651,\n",
       " '150': 637,\n",
       " '153': 3150,\n",
       " 'half8th': 4893,\n",
       " 'textbook': 5783,\n",
       " \"''\": 545,\n",
       " 'exist': 4467,\n",
       " 'accounting': 7498,\n",
       " 'ericsson': 3037,\n",
       " 'dealer': 7737,\n",
       " 'norm150p': 1558,\n",
       " '80160': 5262,\n",
       " 'floor': 2327,\n",
       " 'actor': 3077,\n",
       " 'uttered': 7625,\n",
       " 'flood': 7423,\n",
       " 'role': 1627,\n",
       " 'ambitious': 6613,\n",
       " 'smell': 5050,\n",
       " 'truffles': 3145,\n",
       " \"'t\": 2091,\n",
       " 'intend': 5788,\n",
       " 'fathima': 2739,\n",
       " '07742676969': 3002,\n",
       " 'outage': 7535,\n",
       " 'mre': 8322,\n",
       " 'hollalater': 5070,\n",
       " 'jewelry': 7784,\n",
       " 'nxt': 1400,\n",
       " 'loveme': 3308,\n",
       " 'preponed': 8426,\n",
       " 'cuddled': 6544,\n",
       " '07732584351': 4390,\n",
       " 'broadband': 7211,\n",
       " 'time': 22,\n",
       " 'push': 3909,\n",
       " 'timi': 7403,\n",
       " '6230': 6371,\n",
       " 'sday': 8525,\n",
       " 'chain': 2650,\n",
       " 'saibaba': 8770,\n",
       " 'cudnt': 5166,\n",
       " '3ss': 3241,\n",
       " 'boltblue': 4698,\n",
       " 'oso': 500,\n",
       " 'baller': 8392,\n",
       " \"when's\": 2859,\n",
       " 'overdid': 8567,\n",
       " 'lara': 6034,\n",
       " 'macha': 4885,\n",
       " 'comuk': 1130,\n",
       " 'followin': 4622,\n",
       " 'macho': 2913,\n",
       " 'machi': 4677,\n",
       " 'jeri': 4897,\n",
       " 'k718': 6138,\n",
       " 'prepaid': 3035,\n",
       " 'doke': 6266,\n",
       " 'minuts': 1860,\n",
       " 'cheap': 1034,\n",
       " 'maretare': 7017,\n",
       " \"tyler's\": 8296,\n",
       " 'choice': 1811,\n",
       " 'onwords': 8438,\n",
       " 'pleassssssseeeeee': 4521,\n",
       " '5min': 2355,\n",
       " 'exact': 2131,\n",
       " '28days': 3449,\n",
       " 'minute': 600,\n",
       " 'tear': 1440,\n",
       " 'leave': 191,\n",
       " 'solved': 4032,\n",
       " 'settle': 2835,\n",
       " 'team': 1283,\n",
       " 'loads': 925,\n",
       " 'prevent': 4866,\n",
       " 'spiritual': 8798,\n",
       " 'rents': 3693,\n",
       " 'videochat': 1788,\n",
       " 'sigh': 4060,\n",
       " 'prediction': 4508,\n",
       " 'sign': 1191,\n",
       " '08712402972': 7077,\n",
       " 'erotic': 8643,\n",
       " 'shirts': 2864,\n",
       " 'rentl': 1792,\n",
       " 'workand': 6223,\n",
       " 'headset': 7930,\n",
       " \"'hw\": 3879,\n",
       " 'celebrated': 7119,\n",
       " 'melt': 4217,\n",
       " 'current': 1708,\n",
       " '300': 1882,\n",
       " 'axel': 8194,\n",
       " 'falling': 3630,\n",
       " 'ground': 2610,\n",
       " 'boost': 1852,\n",
       " 'unintentionally': 8538,\n",
       " 'funeral': 4197,\n",
       " 'understanding': 2122,\n",
       " 'yards': 8329,\n",
       " 'address': 519,\n",
       " 'alone': 675,\n",
       " 'along': 2159,\n",
       " 'neville': 6857,\n",
       " 'brilliant': 1867,\n",
       " '300603': 3138,\n",
       " 'wherever': 1574,\n",
       " \"anybody's\": 5975,\n",
       " 'bw': 8155,\n",
       " 'fassyole': 8498,\n",
       " 'studies': 8588,\n",
       " 'influx': 8801,\n",
       " 'love': 24,\n",
       " 'prefer': 2795,\n",
       " 'bloody': 1937,\n",
       " 'fake': 3722,\n",
       " '4jx': 4957,\n",
       " 'gotbabes': 6800,\n",
       " 'sky': 1442,\n",
       " 'crammed': 5978,\n",
       " 'working': 399,\n",
       " 'positive': 8735,\n",
       " 'angry': 630,\n",
       " 'tightly': 6817,\n",
       " 'wicket': 8620,\n",
       " 'opposed': 7067,\n",
       " 'wondering': 1140,\n",
       " 'films': 2894,\n",
       " \"cann't\": 2907,\n",
       " 'trishul': 7607,\n",
       " 'loving': 674,\n",
       " '09065394973': 5930,\n",
       " 'afford': 6364,\n",
       " 'ooooooh': 8668,\n",
       " 'appendix': 6793,\n",
       " 'everywhere': 3017,\n",
       " 'ip4': 1164,\n",
       " 'scratches': 5179,\n",
       " 'easiest': 6334,\n",
       " 'behalf': 7970,\n",
       " 'logos': 3516,\n",
       " 'valued': 796,\n",
       " 'hussey': 5034,\n",
       " 'pretend': 7115,\n",
       " 'lttrs': 3873,\n",
       " 'printing': 6526,\n",
       " 'values': 7121,\n",
       " 'following': 1293,\n",
       " 'logon': 8067,\n",
       " 'mesages': 4268,\n",
       " 'muah': 4333,\n",
       " 'awesome': 599,\n",
       " 'weasels': 6685,\n",
       " 'parachute': 5887,\n",
       " '88066': 2812,\n",
       " 'hides': 8191,\n",
       " 'admirer': 1003,\n",
       " 'offense': 8183,\n",
       " 'dooms': 7189,\n",
       " 'poking': 6923,\n",
       " 'meive': 4775,\n",
       " '62220cncl': 7475,\n",
       " 'fps': 6280,\n",
       " 'elephant': 6911,\n",
       " '69200': 6756,\n",
       " 'lido': 2307,\n",
       " 'laundry': 3735,\n",
       " 'landmark': 7893,\n",
       " '23f': 6007,\n",
       " '23g': 6008,\n",
       " 'spot': 7453,\n",
       " 'dats': 4505,\n",
       " 'suntec': 2651,\n",
       " 'unclaimed': 4919,\n",
       " 'date': 538,\n",
       " 'such': 6659,\n",
       " 'data': 5190,\n",
       " 'brainless': 7051,\n",
       " 'disagreeable': 8454,\n",
       " 'stress': 2509,\n",
       " 'surfing': 1698,\n",
       " 'natural': 2772,\n",
       " 'sp': 1183,\n",
       " 'st': 887,\n",
       " 'complaining': 5745,\n",
       " 'si': 2001,\n",
       " 'sh': 2713,\n",
       " 'so': 33,\n",
       " 'sn': 2553,\n",
       " 'swollen': 7836,\n",
       " 'sc': 5726,\n",
       " 'misplaced': 6377,\n",
       " 'sg': 6832,\n",
       " 'hol': 3313,\n",
       " 'se': 2750,\n",
       " 'sd': 5143,\n",
       " 'drunken': 4114,\n",
       " 'bootydelious': 3021,\n",
       " 'differences': 8294,\n",
       " 'speedchat': 3464,\n",
       " 'years': 464,\n",
       " 'professors': 3257,\n",
       " 'course': 771,\n",
       " 'studentfinancial': 7091,\n",
       " 'konw': 3600,\n",
       " 'disconnect': 4172,\n",
       " 'jia': 3618,\n",
       " 'avin': 8065,\n",
       " 'attraction': 5221,\n",
       " 'jiu': 1711,\n",
       " '930': 3276,\n",
       " 'decades': 8129,\n",
       " 'instantly': 2823,\n",
       " 'conveying': 7720,\n",
       " 'matches': 1236,\n",
       " 'smarter': 4399,\n",
       " 'n9dx': 3106,\n",
       " 'feelin': 3663,\n",
       " 'records': 1770,\n",
       " 'subscribers': 7926,\n",
       " 'sorted': 2637,\n",
       " 'twilight': 3659,\n",
       " 'maintaining': 8660,\n",
       " 'matched': 5519,\n",
       " 'pokkiri': 5220,\n",
       " 'shouted': 2513,\n",
       " '83435': 6424,\n",
       " 'blacko': 8499,\n",
       " 'othrwise': 7885,\n",
       " 'quarter': 8228,\n",
       " 'ovr': 5531,\n",
       " 'retrieve': 3846,\n",
       " 'padhe': 6305,\n",
       " 'receipt': 1802,\n",
       " 'disasters': 6955,\n",
       " 'pataistha': 7353,\n",
       " 'joker': 8286,\n",
       " '83332': 7069,\n",
       " 'blu': 4205,\n",
       " 'alwa': 6827,\n",
       " '83383': 3935,\n",
       " 'wiskey': 3890,\n",
       " 'trauma': 4991,\n",
       " 'internet': 1308,\n",
       " 'hcl': 6357,\n",
       " 'flurries': 7522,\n",
       " 'ppt150x3': 5078,\n",
       " 'sheffield': 3407,\n",
       " '1131': 6985,\n",
       " 'million': 7804,\n",
       " 'possibility': 7552,\n",
       " 'quite': 323,\n",
       " 'grandma': 8009,\n",
       " 'vijaykanth': 8122,\n",
       " 'raed': 3610,\n",
       " 'training': 1249,\n",
       " 'thankyou': 6115,\n",
       " 'rael': 3602,\n",
       " 'dunno': 377,\n",
       " 'swtheart': 2384,\n",
       " 'initiate': 6781,\n",
       " 'massive': 4271,\n",
       " 'dinero': 5743,\n",
       " 'neglect': 4903,\n",
       " '20m12aq': 6540,\n",
       " 'emotion': 7883,\n",
       " 'oni': 1885,\n",
       " 'frwd': 6233,\n",
       " 'spoken': 2056,\n",
       " 'potter': 3523,\n",
       " 'one': 29,\n",
       " 'spanish': 2328,\n",
       " 'vava': 2313,\n",
       " '69911': 5280,\n",
       " 'open': 652,\n",
       " 'city': 1761,\n",
       " 'sozi': 7975,\n",
       " 'bite': 2483,\n",
       " 'uks': 2866,\n",
       " 'indicate': 4243,\n",
       " 'fml': 1912,\n",
       " '2': 4,\n",
       " 'stifled': 6065,\n",
       " 'stuffed': 5097,\n",
       " 'definitly': 7991,\n",
       " 'bits': 4979,\n",
       " 'coccooning': 6924,\n",
       " 'floppy': 7951,\n",
       " 'snatch': 5329,\n",
       " 'fooled': 5676,\n",
       " 'boyfriend': 3626,\n",
       " 'remembr': 3522,\n",
       " 'depressed': 3458,\n",
       " 'scrumptious': 4891,\n",
       " 'cutefrnd': 2382,\n",
       " 'arun': 2732,\n",
       " 'arul': 5941,\n",
       " 'attracts': 7332,\n",
       " 'illness': 4346,\n",
       " 'sao': 4449,\n",
       " 'sam': 1683,\n",
       " 'sac': 4104,\n",
       " 'turned': 4980,\n",
       " 'argument': 1684,\n",
       " 'sae': 557,\n",
       " 'sad': 563,\n",
       " 'woah': 8188,\n",
       " 'say': 109,\n",
       " 'sar': 3469,\n",
       " 'saw': 475,\n",
       " 'sat': 360,\n",
       " '1cup': 7853,\n",
       " 'zoe': 3408,\n",
       " 'babysit': 7037,\n",
       " 'aproach': 3875,\n",
       " '15pm': 7982,\n",
       " 'note': 2243,\n",
       " 'taka': 8311,\n",
       " 'algarve': 3005,\n",
       " 'take': 54,\n",
       " 'wanting': 2087,\n",
       " 'ericson': 6507,\n",
       " 'handing': 6527,\n",
       " 'printer': 6642,\n",
       " 'opposite': 8219,\n",
       " 'knew': 905,\n",
       " 'buffet': 2935,\n",
       " 'printed': 3047,\n",
       " 'pages': 2218,\n",
       " 'countinlots': 6893,\n",
       " '02085076972': 7266,\n",
       " 'phil': 6856,\n",
       " 'infections': 4316,\n",
       " 'drive': 585,\n",
       " 'werethe': 4652,\n",
       " 'salt': 8552,\n",
       " 'annoncement': 4471,\n",
       " 'walking': 1418,\n",
       " '5wq': 6494,\n",
       " 'inclu': 8787,\n",
       " 'bright': 2148,\n",
       " \"joke's\": 7724,\n",
       " '5wb': 1417,\n",
       " '5we': 1165,\n",
       " 'applyed': 8093,\n",
       " 'slow': 953,\n",
       " 'farting': 8360,\n",
       " 'robs': 6622,\n",
       " 'coaxing': 5173,\n",
       " 'foward': 8595,\n",
       " 'jaykwon': 4680,\n",
       " 'going': 30,\n",
       " 'actin': 4380,\n",
       " 'hockey': 2967,\n",
       " 'slob': 6794,\n",
       " 'caroline': 3095,\n",
       " 'carolina': 8267,\n",
       " 'b4u': 6059,\n",
       " 'psychiatrist': 5623,\n",
       " '4882': 3229,\n",
       " 'freezing': 2703,\n",
       " 'murali': 5836,\n",
       " 'compliments': 8778,\n",
       " 'awaiting': 1260,\n",
       " 'settings': 2186,\n",
       " 'getstop': 3300,\n",
       " 'borrow': 3306,\n",
       " 'tenerife': 1349,\n",
       " 'worried': 983,\n",
       " 'racal': 5057,\n",
       " 'priest': 6847,\n",
       " 'roger': 2229,\n",
       " 'worries': 1095,\n",
       " 'tortilla': 3001,\n",
       " 'where': 203,\n",
       " 'xmas': 435,\n",
       " 'busetop': 4636,\n",
       " 'persian': 8254,\n",
       " 'anyways': 1939,\n",
       " 'pisces': 8616,\n",
       " \"alex's\": 4864,\n",
       " 'cttargg': 5040,\n",
       " '8883': 8068,\n",
       " 'dormitory': 8466,\n",
       " 'x29': 6935,\n",
       " 'refreshed': 4602,\n",
       " 'availa': 4951,\n",
       " 'jobs': 3916,\n",
       " 'screen': 2653,\n",
       " \"employer's\": 4578,\n",
       " 'concentrate': 2596,\n",
       " 'spare': 3285,\n",
       " 'amore': 4361,\n",
       " 'spark': 5640,\n",
       " 'listening2the': 5370,\n",
       " 'many': 181,\n",
       " 's': 207,\n",
       " 'residency': 6382,\n",
       " '7876150ppm': 4008,\n",
       " 'expression': 3135,\n",
       " \"can't\": 173,\n",
       " 'stream': 6246,\n",
       " 'conected': 5230,\n",
       " 'call2optout': 888,\n",
       " 'anti': 2628,\n",
       " '3000': 7562,\n",
       " 'mapquest': 7963,\n",
       " 'boat': 2763,\n",
       " 'cramps': 2714,\n",
       " 'swashbuckling': 8282,\n",
       " 'stretch': 3556,\n",
       " 'west': 3983,\n",
       " 'breath': 3443,\n",
       " 'reflex': 5920,\n",
       " 'wants': 561,\n",
       " 'gist': 3279,\n",
       " 'hlday': 7689,\n",
       " 'coughing': 7214,\n",
       " '09111032124': 4641,\n",
       " '820554ad0a1705572711': 8731,\n",
       " 'photos': 1782,\n",
       " '300p': 3829,\n",
       " '09058097218': 5189,\n",
       " 'naseeb': 6203,\n",
       " 'single': 1222,\n",
       " 'squeezed': 8659,\n",
       " 'situation': 1066,\n",
       " '3uz': 2387,\n",
       " 'ive': 940,\n",
       " 'wire3': 7230,\n",
       " 'purse': 3494,\n",
       " 'bros': 2858,\n",
       " 'blah': 2424,\n",
       " 'limping': 8230,\n",
       " 'verified': 4175,\n",
       " '0125698789': 4531,\n",
       " 'thinkin': 1147,\n",
       " 'cost1': 1620,\n",
       " 'cost3': 7450,\n",
       " 'thirtyeight': 4519,\n",
       " 'downs': 6691,\n",
       " 'sterling': 6046,\n",
       " 'askin': 1514,\n",
       " 'sickness': 7651,\n",
       " 'mtnl': 8439,\n",
       " 'cheers': 1159,\n",
       " 'callon': 6652,\n",
       " 'cheery': 5381,\n",
       " 'italian': 1805,\n",
       " 'defo': 5182,\n",
       " '88888': 4013,\n",
       " 'natalie2k9': 8428,\n",
       " 'costa': 1433,\n",
       " 'volcanoes': 6949,\n",
       " 'nothin': 3848,\n",
       " \"shahjahan's\": 7248,\n",
       " 'costs': 1834,\n",
       " '1winaweek': 2776,\n",
       " 'grumpy': 4435,\n",
       " 'hubby': 3588,\n",
       " 'dimension': 5698,\n",
       " 'summer': 1128,\n",
       " 'being': 8636,\n",
       " '150p16': 3433,\n",
       " 'forevr': 2917,\n",
       " 'sum1': 2306,\n",
       " '08714712412': 8409,\n",
       " '88088': 3867,\n",
       " '6089': 8658,\n",
       " 'lolnice': 4692,\n",
       " 'ghodbandar': 5724,\n",
       " 'weekly': 486,\n",
       " '81151': 2052,\n",
       " '310303': 6900,\n",
       " 'kerala': 1578,\n",
       " 'adsense': 8420,\n",
       " 'drugdealer': 6005,\n",
       " 'f4q': 4604,\n",
       " 'starving': 6970,\n",
       " 'proze': 8011,\n",
       " 'aspects': 6427,\n",
       " 'around': 167,\n",
       " 'lnly': 5681,\n",
       " 'pos': 6486,\n",
       " 'dark': 2870,\n",
       " 'traffic': 3678,\n",
       " 'pop': 2495,\n",
       " '2geva': 4030,\n",
       " 'world': 303,\n",
       " 'postal': 5241,\n",
       " 'vague': 7497,\n",
       " 'dare': 2239,\n",
       " 'stranger': 2398,\n",
       " 'poo': 7394,\n",
       " 'quizclub': 6400,\n",
       " 'alaipayuthe': 3696,\n",
       " 'gimme': 2517,\n",
       " 'clas': 8278,\n",
       " 'gimmi': 7460,\n",
       " 'slovely': 5725,\n",
       " 'masteriastering': 7149,\n",
       " 'ortxt': 8361,\n",
       " '4few': 5229,\n",
       " 'monthlysubscription': 4720,\n",
       " 'playin': 7537,\n",
       " '5wkg': 5142,\n",
       " 'thinks': 773,\n",
       " \"there'll\": 8176,\n",
       " 'strewn': 5857,\n",
       " 'memories': 8213,\n",
       " 'noon': 1121,\n",
       " \"''ok''\": 1859,\n",
       " ...}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...,   20 4361   98]\n",
      " [   0    0    0 ...,  422    2 1885]\n",
      " [   0    0    0 ...,  618  343 2936]\n",
      " ..., \n",
      " [   0    0    0 ...,   33  504 8817]\n",
      " [   0    0    0 ...,  993  151   12]\n",
      " [   0    0    0 ...,   88  436  219]]\n"
     ]
    }
   ],
   "source": [
    "print(data_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Split the data into train, test and validation sets\n",
    "#First split train and test parts, then split train part to train and validation parts\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_sequences, labels, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Vocabulary size plus one for 0, the int number that added for padding\n",
    "n_input = len(word_index)+1\n",
    "#number of units\n",
    "num_hidden = 256\n",
    "lstm_layer_numbers=2\n",
    "embed_size=300\n",
    "batch_size= 250\n",
    "learning_rate=0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and execute the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 1, 'cost_train=', 0.24554018889154705, 'cost_val=', 0.24357138574123383)\n",
      "('acc_train=', 0.51861050086362026, 'acc_val=', 0.56733236710230517)\n",
      "('Epoch:', 2, 'cost_train=', 0.24271440931728908, 'cost_val=', 0.24055684109528858)\n",
      "('acc_train=', 0.53575725214821945, 'acc_val=', 0.59855898221333825)\n",
      "('Epoch:', 3, 'cost_train=', 0.24021922264780318, 'cost_val=', 0.23756549259026843)\n",
      "('acc_train=', 0.56210471051079891, 'acc_val=', 0.61807562907536828)\n",
      "('Epoch:', 4, 'cost_train=', 0.2359522304364613, 'cost_val=', 0.23460867007573449)\n",
      "('acc_train=', 0.58719751664570397, 'acc_val=', 0.64051973819732666)\n",
      "('Epoch:', 5, 'cost_train=', 0.23432466813496178, 'cost_val=', 0.23166404167811078)\n",
      "('acc_train=', 0.59472536614962979, 'acc_val=', 0.65808471043904626)\n",
      "('Epoch:', 6, 'cost_train=', 0.23178069187062125, 'cost_val=', 0.22874727348486579)\n",
      "('acc_train=', 0.61040836998394554, 'acc_val=', 0.67272218068440759)\n",
      "('Epoch:', 7, 'cost_train=', 0.22912079521587916, 'cost_val=', 0.22583048542340595)\n",
      "('acc_train=', 0.63152814337185437, 'acc_val=', 0.68931134541829431)\n",
      "('Epoch:', 8, 'cost_train=', 0.22575594484806055, 'cost_val=', 0.22296227514743805)\n",
      "('acc_train=', 0.64846579517636971, 'acc_val=', 0.70980376005172729)\n",
      "('Epoch:', 9, 'cost_train=', 0.222772801561015, 'cost_val=', 0.22011037170886993)\n",
      "('acc_train=', 0.66373058727809364, 'acc_val=', 0.71663461128870654)\n",
      "('Epoch:', 10, 'cost_train=', 0.22008774748870302, 'cost_val=', 0.21726927161216736)\n",
      "('acc_train=', 0.67209486024720344, 'acc_val=', 0.72639292478561401)\n",
      "('Epoch:', 11, 'cost_train=', 0.21665703931025096, 'cost_val=', 0.21446812649567926)\n",
      "('acc_train=', 0.68756876247269783, 'acc_val=', 0.73029625415802002)\n",
      "('Epoch:', 12, 'cost_train=', 0.21440959615366795, 'cost_val=', 0.21168354153633118)\n",
      "('acc_train=', 0.70534283774239681, 'acc_val=', 0.74005454778671265)\n",
      "('Epoch:', 13, 'cost_train=', 0.21134558213608606, 'cost_val=', 0.20892044405142463)\n",
      "('acc_train=', 0.71893477865627831, 'acc_val=', 0.74883703390757239)\n",
      "('Epoch:', 14, 'cost_train=', 0.20871646382978984, 'cost_val=', 0.20619597037633258)\n",
      "('acc_train=', 0.71851656266621189, 'acc_val=', 0.75566786527633678)\n",
      "('Epoch:', 15, 'cost_train=', 0.20618469268083575, 'cost_val=', 0.20351825654506683)\n",
      "('acc_train=', 0.73273581692150647, 'acc_val=', 0.75859534740448009)\n",
      "('Epoch:', 16, 'cost_train=', 0.20327954739332196, 'cost_val=', 0.20085954666137695)\n",
      "('acc_train=', 0.73273582117898128, 'acc_val=', 0.76542617877324415)\n",
      "('Epoch:', 17, 'cost_train=', 0.20125336732183183, 'cost_val=', 0.19823040068149567)\n",
      "('acc_train=', 0.74486401677131675, 'acc_val=', 0.77128116289774573)\n",
      "('Epoch:', 18, 'cost_train=', 0.19835762573140012, 'cost_val=', 0.19562839965025586)\n",
      "('acc_train=', 0.74862793087959278, 'acc_val=', 0.77616031964619947)\n",
      "('Epoch:', 19, 'cost_train=', 0.19603775867394041, 'cost_val=', 0.19307015836238861)\n",
      "('acc_train=', 0.75594666174479885, 'acc_val=', 0.78103949626286817)\n",
      "('Epoch:', 20, 'cost_train=', 0.19280616513320381, 'cost_val=', 0.190545454621315)\n",
      "('acc_train=', 0.76242898191724506, 'acc_val=', 0.78689448038736987)\n",
      "('Epoch:', 21, 'cost_train=', 0.1914662569761276, 'cost_val=', 0.18804755806922913)\n",
      "('acc_train=', 0.76744753973824631, 'acc_val=', 0.79274948438008641)\n",
      "('Epoch:', 22, 'cost_train=', 0.18796795819486889, 'cost_val=', 0.18560104072093964)\n",
      "('acc_train=', 0.7687021834509713, 'acc_val=', 0.79762862126032519)\n",
      "('Epoch:', 23, 'cost_train=', 0.18627827401672087, 'cost_val=', 0.18317581713199615)\n",
      "('acc_train=', 0.77748466389519821, 'acc_val=', 0.8005561033884685)\n",
      "('Epoch:', 24, 'cost_train=', 0.1827609326158251, 'cost_val=', 0.18078907330830893)\n",
      "('acc_train=', 0.78417608141899109, 'acc_val=', 0.80250777800877882)\n",
      "('Epoch:', 25, 'cost_train=', 0.18095010625464578, 'cost_val=', 0.17846505343914032)\n",
      "('acc_train=', 0.78522161926542, 'acc_val=', 0.80445945262908936)\n",
      "('Epoch:', 26, 'cost_train=', 0.17873163734163555, 'cost_val=', 0.17616929113864899)\n",
      "('acc_train=', 0.78250322171619957, 'acc_val=', 0.80641110738118504)\n",
      "('Epoch:', 27, 'cost_train=', 0.17666805535554886, 'cost_val=', 0.17392137149969739)\n",
      "('acc_train=', 0.7929585661206926, 'acc_val=', 0.80836276213328051)\n",
      "('Epoch:', 28, 'cost_train=', 0.17375171610287257, 'cost_val=', 0.17171377936999002)\n",
      "('acc_train=', 0.79337678636823394, 'acc_val=', 0.81129024426142382)\n",
      "('Epoch:', 29, 'cost_train=', 0.17251241632870265, 'cost_val=', 0.16953934729099274)\n",
      "('acc_train=', 0.7942132055759431, 'acc_val=', 0.81226609150568652)\n",
      "('Epoch:', 30, 'cost_train=', 0.16991218605211802, 'cost_val=', 0.16740019619464874)\n",
      "('acc_train=', 0.79776802233287269, 'acc_val=', 0.81519359350204468)\n",
      "('Epoch:', 31, 'cost_train=', 0.16761794473443714, 'cost_val=', 0.1653110682964325)\n",
      "('acc_train=', 0.7971407004765102, 'acc_val=', 0.81714524825414014)\n",
      "('Epoch:', 32, 'cost_train=', 0.16544095426797864, 'cost_val=', 0.16327637930711109)\n",
      "('acc_train=', 0.80299569453511921, 'acc_val=', 0.81812107563018799)\n",
      "('Epoch:', 33, 'cost_train=', 0.16325736152274267, 'cost_val=', 0.161280686656634)\n",
      "('acc_train=', 0.81115085312298363, 'acc_val=', 0.81909690300623583)\n",
      "('Epoch:', 34, 'cost_train=', 0.16128814859049662, 'cost_val=', 0.15932872394720715)\n",
      "('acc_train=', 0.80675961290087006, 'acc_val=', 0.82202438513437892)\n",
      "('Epoch:', 35, 'cost_train=', 0.15920092165470123, 'cost_val=', 0.15742885073026019)\n",
      "('acc_train=', 0.80655050703457432, 'acc_val=', 0.82300023237864162)\n",
      "('Epoch:', 36, 'cost_train=', 0.1567035381283079, 'cost_val=', 0.15557403862476349)\n",
      "('acc_train=', 0.81094175577163696, 'acc_val=', 0.82495190699895216)\n",
      "('Epoch:', 37, 'cost_train=', 0.15532660165003367, 'cost_val=', 0.15376163522402445)\n",
      "('acc_train=', 0.8136601405484335, 'acc_val=', 0.82787938912709547)\n",
      "('Epoch:', 38, 'cost_train=', 0.1535275887165751, 'cost_val=', 0.15198518335819244)\n",
      "('acc_train=', 0.81282370856830044, 'acc_val=', 0.82885521650314331)\n",
      "('Epoch:', 39, 'cost_train=', 0.15205963275262288, 'cost_val=', 0.15026083588600159)\n",
      "('acc_train=', 0.81596031785011292, 'acc_val=', 0.83178271849950147)\n",
      "('Epoch:', 40, 'cost_train=', 0.14994364551135478, 'cost_val=', 0.14858630299568176)\n",
      "('acc_train=', 0.81679673705782219, 'acc_val=', 0.83373439311981201)\n",
      "('Epoch:', 41, 'cost_train=', 0.14764434418507988, 'cost_val=', 0.14695695042610168)\n",
      "('acc_train=', 0.81867869836943485, 'acc_val=', 0.83373439311981201)\n",
      "('Epoch:', 42, 'cost_train=', 0.14648550961698803, 'cost_val=', 0.14535408218701679)\n",
      "('acc_train=', 0.8172149445329393, 'acc_val=', 0.83373439311981201)\n",
      "('Epoch:', 43, 'cost_train=', 0.14448414742946625, 'cost_val=', 0.14379953344662985)\n",
      "('acc_train=', 0.82432457378932411, 'acc_val=', 0.83373439311981201)\n",
      "('Epoch:', 44, 'cost_train=', 0.14273720660379954, 'cost_val=', 0.14228995144367218)\n",
      "('acc_train=', 0.8213970831462315, 'acc_val=', 0.83373439311981201)\n",
      "('Epoch:', 45, 'cost_train=', 0.1413744954126222, 'cost_val=', 0.14082573354244232)\n",
      "('acc_train=', 0.82537011589322762, 'acc_val=', 0.83471024036407471)\n",
      "('Epoch:', 46, 'cost_train=', 0.14006084416593823, 'cost_val=', 0.13940320909023285)\n",
      "('acc_train=', 0.82599743774959011, 'acc_val=', 0.83471024036407471)\n",
      "('Epoch:', 47, 'cost_train=', 0.13887955886977058, 'cost_val=', 0.13801426192124686)\n",
      "('acc_train=', 0.82704296708106995, 'acc_val=', 0.83568606774012255)\n",
      "('Epoch:', 48, 'cost_train=', 0.13711546255009516, 'cost_val=', 0.13666487236817679)\n",
      "('acc_train=', 0.82892493265015743, 'acc_val=', 0.83568606774012255)\n",
      "('Epoch:', 49, 'cost_train=', 0.13503379481179373, 'cost_val=', 0.13536509871482849)\n",
      "('acc_train=', 0.83164331742695408, 'acc_val=', 0.83568606774012255)\n",
      "('Epoch:', 50, 'cost_train=', 0.13391311147383284, 'cost_val=', 0.13409578303496045)\n",
      "('acc_train=', 0.83017956359045852, 'acc_val=', 0.83568606774012255)\n"
     ]
    }
   ],
   "source": [
    "inputs,targets,keep_prob=create_model_inputs()\n",
    "embeds=build_embeddings(inputs,n_input,embed_size)\n",
    "initial_state, stacked_lstm_cells = build_RNN(inputs,num_hidden,lstm_layer_numbers,keep_prob,batch_size)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(stacked_lstm_cells, embeds, initial_state=initial_state)\n",
    "# Loss and optimizer\n",
    "#second parameter: one output which indicates if the input message is spam or ham\n",
    "predictions = tf.contrib.layers.fully_connected(outputs[:, -1], 1, activation_fn=tf.sigmoid,\n",
    "                                                weights_initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                                biases_initializer=tf.zeros_initializer())\n",
    "\n",
    "loss_function = tf.losses.mean_squared_error(targets, predictions)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate).minimize(loss_function)\n",
    "correct_pred = tf.equal(tf.cast(tf.round(predictions), tf.int32), labels)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))    \n",
    "\n",
    "#Since we put both train and validation in the same session, we define initial_state for each separately\n",
    "initial_state_train=initial_state\n",
    "initial_state_val=initial_state\n",
    "#Execute the default graph\n",
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "init_op = tf.initialize_all_variables()\n",
    "sess.run(init_op)\n",
    "no_of_batches_train = int(len(X_train)/batch_size)\n",
    "no_of_batches_valid = int(len(X_val)/batch_size)\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    state = sess.run(initial_state_train)\n",
    "    avg_cost_train = 0 \n",
    "    avg_acc_train= 0\n",
    "    for ii, (x, y) in enumerate(get_batches(X_train, y_train, batch_size), 1):\n",
    "        _, cost, acc, state= sess.run([optimizer, loss_function,accuracy, final_state], feed_dict={inputs: x,\n",
    "                                                        targets: y[:, None],keep_prob: 0.5,initial_state_train: state})\n",
    "        \n",
    "        avg_cost_train += cost / no_of_batches_train\n",
    "        avg_acc_train += acc / no_of_batches_train\n",
    "    state_val = sess.run(initial_state_val)\n",
    "    avg_cost_val = 0  \n",
    "    avg_acc_val = 0\n",
    "    for ii, (x, y) in enumerate(get_batches(X_val, y_val, batch_size), 1):\n",
    "        _, cost, acc, state_val= sess.run([optimizer, loss_function, accuracy, final_state], feed_dict={inputs: x,\n",
    "                                                        targets: y[:, None],keep_prob: 1,initial_state_val: state_val})\n",
    "        \n",
    "        avg_cost_val += cost / no_of_batches_valid\n",
    "        avg_acc_val += acc / no_of_batches_valid\n",
    "    print(\"Epoch:\", epoch+1, \"cost_train=\", avg_cost_train, \"cost_val=\", avg_cost_val)\n",
    "    print(\"acc_train=\", avg_acc_train, \"acc_val=\", avg_acc_val) \n",
    "#Save the model into a file \n",
    "checkpoint=\"./model/savedmodel.ckpt\"\n",
    "save_path = saver.save(sess, checkpoint)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/savedmodel.ckpt\n",
      "('Test loss', 0.12515557184815407)\n",
      "('Test Accuracy', 0.84251686930656433)\n"
     ]
    }
   ],
   "source": [
    "#Test the saved model\n",
    "no_of_batches_test = int(len(X_test)/batch_size)\n",
    "sess = tf.Session()\n",
    "#Load the model\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, checkpoint)\n",
    "state_test = sess.run(initial_state)\n",
    "avg_cost_test = 0  \n",
    "avg_acc_test = 0  \n",
    "for ii, (x, y) in enumerate(get_batches(X_test, y_test, batch_size), 1):\n",
    "    _, cost, acc, state_test = sess.run([optimizer, loss_function, accuracy, final_state], feed_dict={inputs: x,\n",
    "                                                    targets: y[:, None],keep_prob: 1,initial_state: state_test})\n",
    "    avg_cost_test += cost / no_of_batches_test\n",
    "    avg_acc_test += acc / no_of_batches_test\n",
    "print(\"Test loss\",avg_cost_test) \n",
    "print(\"Test Accuracy\",avg_acc_test)\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
